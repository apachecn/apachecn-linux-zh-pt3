# 五、高级设置

如前所述，您可以将热备盘分配给池。如果 ZFS 池丢失一个磁盘，将自动连接备用磁盘，并启动重新同步过程。

让我们考虑一个镜像池，每个池包含两个 vdevs 和两个驱动器。为了清楚起见，它将是四个硬盘。它们将被成对分组，并且每一对将在内部镜像内容。如果我们有驱动器 A、B、C 和 D，驱动器 A 和 B 将是一个镜像对，驱动器 C 和 D 将是第二个镜像对:

```sh
trochej@ubuntuzfs:~$ sudo zpool status

  pool: datapool
 state: ONLINE
  scan: none requested
 config:

    NAME        STATE     READ WRITE CKSUM
    datapool    ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
        sdb     ONLINE       0     0     0
        sdc     ONLINE       0     0     0
      mirror-1  ONLINE       0     0     0
        sdd     ONLINE       0     0     0
        sde     ONLINE       0     0     0

errors: No known data errors

```

您可以通过运行`zpool add spare`命令来添加热备盘设备:

```sh
trochej@ubuntuzfs:~$ sudo zpool add datapool -f spare /dev/sdf

```

接下来，通过查询池的状态来确认磁盘已添加:

```sh
trochej@ubuntuzfs:~$ sudo zpool status datapool

  pool: datapool
 state: ONLINE
  scan: none requested
 config:

    NAME        STATE     READ WRITE CKSUM
    datapool    ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
        sdb     ONLINE       0     0     0
        sdc     ONLINE       0     0     0
      mirror-1  ONLINE       0     0     0
        sdd     ONLINE       0     0     0
        sde     ONLINE       0     0     0
    spares
      sdf       AVAIL

errors: No known data errors

```

如果您想从池中移除备件，请使用`zpool remove`命令:

```sh
trochej@ubuntuzfs:~$ sudo zpool remove datapool /dev/sdf

```

您也可以在这里使用`zpool status`来确认更改。

您可以在多个池之间共享一个热备盘。您可以创建一个镜像池来托管非常重要的数据或需要快速传输的数据。然后，您可以创建第二个池 RAIDZ，它需要更多空间，但不是非常关键(仍然是冗余的，但只能丢失一个磁盘)。然后，您可以为这两个池分配一个热备盘。出现故障的设备将占用热备盘设备，然后该设备将无法用于第二个池，直到它被释放。

Note

使用热备盘有一个重要的注意事项。如果您以最大限度减少硬件故障影响的方式规划池中的驱动器，则热备盘的放置方式可能不是让您保持该质量的最佳方式。对于共享热备盘来说尤其如此。我见过的许多实际安装都使用了备用驱动器。它们被放置在机箱中，以确保在大多数情况下具有最佳的硬件故障恢复能力。当池中的驱动器出现故障时，系统管理员会从监控系统收到警报，然后手动更换驱动器。

## ZIL 装置

ZIL 代表 ZFS 意向日志。它是数据块中永久存储写缓存的部分。通常，ZFS 会从存储池本身分配一些数据块。但是，由于池很忙并且位于旋转的磁盘上，性能可能不令人满意。

为了更好地适应性能要求，可以将 ZIL(也称为 SLOG)移到单独的设备上。该设备必须能够持续启动，这样，突然断电并不意味着事务数据丢失。对于基于 RAM 的设备，它们必须由电池或电容供电。也可以使用 SSD 设备。

《ZFS 管理指南》建议 ZIL 不低于 64 MB(这是对 ZFS 使用的任何设备的硬性要求),最多不超过可用 RAM 的一半。因此，对于 32 GB 的内存，应该使用 16 GB 的 ZIL 设备。实际上，我很少见过大于 32 GB 的，8 或 16 GB 是最常见的情况。原因是这是一个写缓冲区。将刷新到硬盘驱动器的写入将在 ZIL 中进行分组，以减少物理操作和碎片。一旦达到阈值，这些分组的更改将被写入物理驱动器。给它一个快速设备，最好是一个 RAM 设备，允许这些操作非常快，并大大加快写入速度。这还允许您转移通常会利用池带宽的 I/O(写入 ZIL ),为池本身提供一些额外的性能。

要添加 ZIL 设备，请先确认您的池运行状况良好。它还会提醒您哪些驱动器属于 ZFS 池:

```sh
root@xubuntu:~# zpool status
 pool: data
state: ONLINE
 scan: none requested
config:

       NAME        STATE     READ WRITE CKSUM
       data        ONLINE       0     0     0
         mirror-0  ONLINE       0     0     0
           sdb     ONLINE       0     0     0
           sdc     ONLINE       0     0     0
         mirror-1  ONLINE       0     0     0
           sdd     ONLINE       0     0     0
           sde     ONLINE       0     0     0
         mirror-2  ONLINE       0     0     0
           sdf     ONLINE       0     0     0
           sdg     ONLINE       0     0     0

errors: No known data errors

 pool: rpool
state: ONLINE
 scan: none requested
config:

       NAME          STATE     READ WRITE CKSUM
       rpool         ONLINE       0     0     0
         root_crypt  ONLINE       0     0     0

errors: No known data errors

```

添加`/dev/sdh`和`/dev/sdi`驱动器作为镜像日志设备:

```sh
root@xubuntu:~# zpool add -f data log mirror /dev/sdh /dev/sdi

```

虽然 L2ARC 的内容(将在下一节中介绍)并不重要，但是 ZIL 保存了有关磁盘上的数据如何变化的信息。丢失 ZIL 不会使 ZFS 文件系统损坏，但可能会导致一些更改丢失。因此镜像。

通过运行`zpool status`确认更改生效:

```sh
root@xubuntu:~# zpool status data
 pool: data
state: ONLINE
 scan: none requested
config:

       NAME        STATE     READ WRITE CKSUM 

       data        ONLINE       0     0     0
         mirror-0  ONLINE       0     0     0
           sdb     ONLINE       0     0     0
           sdc     ONLINE       0     0     0
         mirror-1  ONLINE       0     0     0
           sdd     ONLINE       0     0     0
           sde     ONLINE       0     0     0
         mirror-2  ONLINE       0     0     0
           sdf     ONLINE       0     0     0
           sdg     ONLINE       0     0     0
       logs
         mirror-3  ONLINE       0     0     0
           sdh     ONLINE       0     0     0
           sdi     ONLINE       0     0     0

errors: No known data errors

```

您的新日志设备是 mirror-3。

## L2ARC 设备(缓存)

ZFS 采用了一种叫做自适应替换缓存的缓存技术。简而言之，它基于最近最少使用(LRU)算法，该算法跟踪每个缓存页面的访问时间。然后从最近使用到最近最少使用对它们进行排序。当添加新的头部时，列表的尾部被驱逐。

ARC 通过跟踪两个列表中的页面——最近使用的和最频繁使用的——来改进这个算法。技术细节在这里并不重要，但是可以说，基于 ARC 的缓存的效率通常比 LRU 好得多。

导入池时，ARC 始终存在于操作系统的内存中。顺便提一下，如果您监视您的 RAM 并看到它的大部分正在被使用，不要惊慌。有这么一句话，“不用的内存就是浪费的内存”。你的操作系统试图在内存中尽可能多的存储空间，以减少磁盘操作。如你所知，磁盘是计算机中最慢的部分，即使是现代的固态硬盘。您应该注意的是，使用的 RAM 中有多少是缓存和缓冲区，有多少用于运行进程。

对于非常繁忙的服务器，将尽可能多的数据从驱动器加载到内存是很有意义的，因为这可以大大加快操作速度。

从 RAM 读取数据至少比从硬盘读取数据快 10 倍。但是，如果您的内存资源有限，但仍然希望尽可能多地缓存，会发生什么情况呢？

将一些 SSD 驱动器放入您的服务器，并将其用作 L2ARC 设备。L2 弧是二级弧。这些页面通常会被从缓存中逐出，因为 RAM 太小。但是，由于它们再次被请求的可能性仍然很高，因此它们可能被放置在中间区域，在快速 SSD 驱动器上。

因此，将 L2ARCs 放在镜像 SSD 上是非常有意义的。

要将`/dev/sdi`作为缓存设备放入池中，请运行以下命令:

```sh
root@xubuntu:~# zpool add -f data cache /dev/sdi

```

确认它有效:

```sh
root@xubuntu:~# zpool status data
 pool: data
state: ONLINE
 scan: none requested
config:

       NAME        STATE     READ WRITE CKSUM
       data        ONLINE       0     0     0
         mirror-0  ONLINE       0     0     0
           sdb     ONLINE       0     0     0
           sdc     ONLINE       0     0     0
         mirror-1  ONLINE       0     0     0
           sdd     ONLINE       0     0     0
           sde     ONLINE       0     0     0
         mirror-2  ONLINE       0     0     0
           sdf     ONLINE       0     0     0
           sdg     ONLINE       0     0     0
       logs
         sdh       ONLINE       0     0     0
       cache
         sdi       ONLINE       0     0     0

errors: No known data errors

```

## 配额和保留

在正常操作中，池中的每个文件系统都可以自由占用可用空间，直至达到整个池的容量，直到用完为止。唯一的限制是其他文件系统也占用空间。在这方面，对于 ZFS，您不应该考虑文件系统的容量，而应该考虑总的池空间。

然而，在某些情况下，当文件系统被限制在一定的空间内或者保证有足够的空间供自己使用时，您需要模拟传统的文件系统行为。

让我们考虑一个在普通磁盘分区上创建的传统文件系统。如果分区被创建为 3 GB，则文件系统自身的空间不会少于也不会多于 3 GB。如果您将它挂载为，比方说，`/var/log`，那么您系统中的日志将拥有全部 3 GB 的空间，不会再多了。它们也将与其他文件系统分开。因此，填充`/var/log`目录的日志不会使您的根分区变满，因为它们位于一个单独的空间中。

ZFS 就不是这样了！考虑安装在 ZFS 文件系统上的根目录。假设该池总共有 16 GB 的空间。这适用于`/home`、`/var`和`/var/log`的文件系统。安装完系统后，假设您还有 11 GB 的可用空间。每个文件系统都会消耗这个空间。如果由于某种原因，日志变得混乱——也许某个应用程序切换到了调试模式，而您忘记了它——它们可能会填满这 11 GB 的空间，使所有其他文件系统都处于饥饿状态。在最坏的情况下，您将无法以 root 用户身份登录。

根据您希望如何处理这个问题，您可以采取两种可能的措施:使用配额和使用保留。

配额类似于传统的 Linux 配额，只是它们是针对文件系统而不是系统用户设置的。通过设置配额，您可以防止给定的文件系统增长超过设置的限制。所以如果你想让`/var/log`永远不超过 3 GB，你就给它设置一个配额。

另一方面，保留是对文件系统的保证。通过设置 3 GB 预留空间，您可以保证该给定文件系统至少有 3 GB 的空间，并且池中的其他文件系统不会占用太多空间。

让事情变得稍微复杂一点的是，每种都有两个版本:配额和 refquotas，以及预留和重新预留。我的经验告诉我，这种差异非常重要。

配额和预留量说明了文件系统及其后代使用的存储。这意味着这 3 GB 的空间将是文件系统及其快照和克隆的极限。另一方面，Refquotas 只会跟踪文件系统本身使用的空间。它为一些有趣的场景开辟了道路，在这些场景中，您可以分别为文件系统及其快照设置限制。但是配额带来了一个重要的变化:快照会随着数据的变化而增长。您必须注意快照的大小和增长速度，否则您可能会在预期之前达到配额。

保留和再保留也有同样的风味区别。预留将为文件系统及其后代保证空间，而重新预留将仅为文件系统本身保留该空间。再次提醒，注意，因为你设置的最终结果可能不是你所希望的，也不是你所期望的。

让我们看一些基于真实场景的例子。

您正在运行的服务器有池数据。该池的总容量为 30 TB。该池将由财务、工程和营销部门共享。还将有共享空间，人们可以用来交换文件和愚蠢的猫图片。

这三个部门都向您提供了他们的目录在未来可以增长的规模。财务和营销部门表示，每个项目的容量大约为 5 TB，而工程部门表示，他们预计容量将增长到 10 TB。合起来就是 20 TB，留给你 10 TB 的空闲空间做其他事情。

现在，30 TB 的空间可能看起来是一个很大的数字，对于大多数小型组织来说，可能确实如此。另一方面，工程数据或原始图片和视频(例如，在图形工作室中)可能很快就无法满足需求。

快照是下一小节的主题，但是让我们在这里简单介绍一下。文件系统的快照可以与文件系统在给定时间(即拍摄快照时)的静态映像进行比较。在 ZFS，它可以像任何其他文件系统一样对待，除了它是只读的。这意味着，查看这个文件系统，您将会看到文件和目录在运行`zfs snapshot`命令时的状态。无论运行此命令后这些文件发生了什么情况，您都可以从快照中检索到它们以前的状态。

快照占用的空间量等于数据更改的大小。听起来很复杂，我们来揭开它的神秘面纱。工程有一个 5 GB 的大 CAD 文件。这是一个导入的项目，将继续工作。在它被复制到 ZFS 之后，为了以防万一，拍了一张快照。工程师打开文件，修改了一些东西。保存后，文件的大部分保持不变，但有些地方有所不同。这些差异加起来的大小是 300 MB。这是快照的大小。如果有人删除了文件，快照将增长到 5 GB，因为这是实际文件系统和拍摄快照时刻之间的差异。这背后的机制将在下一节中解释。现在，只要承认这是一个事实。

快照的这种空间消耗在设置预留和配额时起着重要作用。让我们回头看看工程部门的文件系统。该部门估计，他们将存储在文件系统中的数据量将达到 10 TB。但是他们只估计了“原始”数据。文件本身，而不是它们的快照。假设引入项目文件的每日变更量总计为 5 GB。这是一个快照每天占用的空间量，除非它被销毁。为简单起见，假设将只有一个快照，并且它将被永久保存。一年之内，这将相当于从池中占用近 2 TB 的空间！现在，假设您为工程文件系统创建了一个预留空间，并为其分配了 10 TB。你还要加一个配额，11 TB，让他们有喘息的机会，但这样他们就不会饿死其他用户。正如所假设的那样，他们的空间消耗在一年内开始接近 9 TB，突然，距离目标整整少了 2 TB，他们在尝试写入任何内容时都会出现空间不足错误。为了快速解决这种情况，他们删除了一些旧文件，已知这些文件是很久以前最后编辑的，并且存在于几个备份中。显然，他们已经释放了 3 TB 的空间，除了他们不断得到空间不足的错误。因为这个错误，在某些时候他们甚至不能删除文件！

这是预定开始了。问题的第一部分是，随着配额的增长，快照会悄悄地从配额中占用空间。只有当您使用`zpool -o space`命令分析空间消耗时，这一点才显而易见(在其他地方解释)。但是问题的另一部分，即删除东西时违反直觉的空间不足错误，来自于快照本身的性质。当您从文件系统中删除文件时，这些文件会添加到快照中。释放该空间的唯一方法是使用以下命令销毁快照:

```sh
zfs destroy pool/engineering@snapshot

```

现在让我们考虑其他部门。如果您为它们设置了配额，并且它们对文件进行了足够多的编辑，那么由于文件系统快照，它们可能很快就会达到配额。此外，通常会有多个快照。这完全取决于政策制定者，但通常会有每月、每周和每天的快照。有时也有每小时的快照，这取决于一天中数据变化的程度。

现在回到配额和预留以及再配额和再预留之间的区别。第一种跟踪整个使用情况，包括快照。后者只是文件系统。对于工程部门，您可以将 refquota 设置为 11 TB，将配额设置为 13 TB。这将为随着文件删除而增长的快照打开空间，从而提供临时解决方案。然而，没有什么能打败空间利用监控。

配额、保留、引用配额和重新保留是文件系统属性。这意味着使用`zfs set`和`zfs get`命令对它们进行设置和检查。

```sh
root@xubuntu:~# zfs list
NAME                USED  AVAIL  REFER  MOUNTPOINT
data                179K  2.86G    19K  /data
data/engineering     19K  2.86G    19K  /data/engineering

```

要检查数据/工程文件系统上配额、refquota、预留和重新预留的当前值，请运行以下命令:

```sh
root@xubuntu:~# zfs get quota,refquota,reservation,refreservation data/engineering
NAME              PROPERTY        VALUE      SOURCE
data/engineering  quota           none       default
data/engineering  refquota        none       default
data/engineering  reservation     none       default
data/engineering  refreservation  none       default

```

如您所见，它们不是默认设置的。由于我的测试池比考虑的场景小得多，我们将预留设置为 1 GB，配额设置为 1.5 GB，refquota 和 refreservation 稍低一些:

```sh
root@xubuntu:~# zfs set quota=1.5G data/engineering
root@xubuntu:~# zfs set refquota=1G data/engineering
root@xubuntu:~# zfs set reservation=800M data/engineering
root@xubuntu:~# zfs get quota,refquota,reservation data/engineering
NAME              PROPERTY     VALUE     SOURCE
data/engineering  quota        1.50G     local
data/engineering  refquota     1G        local
data/engineering  reservation  800M      local

```

## 快照和克隆

在这里，我们来讨论快照和克隆，ZFS 的两个强大的功能。前面已经讨论过了，所以现在是详细解释的时候了。

如前所述，快照是在给定时间“冻结”文件系统内容的一种方式。由于 ZFS 的“写入时拷贝”特性，创建快照的速度很快(通常只需几分之一秒)，并且只需要很少的处理能力。因此，创建快照作为需要静态内容的长期运行作业的基础是很常见的，例如备份作业。从大型文件系统运行备份作业可能会在不同时间归档文件。通过快照运行它可以保证所有文件都在同一时间被捕获，即使备份运行了几个小时也是如此。此外，如果备份的文件包含需要在备份过程中关闭的应用程序的状态文件，则该应用程序的停机时间可以减少到几分之一秒。

快照的另一个特性是能够将当前文件系统回滚到快照。这意味着管理员可以将所有文件回滚到快照创建时。

ZFS 将更改的数据块写入池中的新位置。因此，除非池被填满，并且旧空间需要回收，否则旧数据块保持不变。因此，快照会自动装载到拍摄快照的文件系统的`.zfs/snapshot`子目录中。例如，对于`data/documents` ZFS 文件系统，如果有一个快照`data/documents@initial`，可以通过查看`/data/documents/.zfs/snapshot/initial`来访问该快照的内容。

可以通过查看上面的目录或运行回滚命令来访问快照内容，这可以有效地将文件系统回滚到快照创建的时刻。这个过程非常快。它只需要更新一些元数据的时间。但是，管理员需要小心谨慎—一旦回滚，文件系统就不能快速恢复到其当前状态。

在有些情况下，只读快照是不够的，将它用作普通文件系统可能会很有用。ZFS 有这样一个特点，它被称为克隆。克隆是快照的读写拷贝。最初，克隆和快照引用同一组字节，因此克隆不消耗任何磁盘空间。当对克隆的内容进行更改时，它开始占用空间。

克隆和从中创建克隆的快照以父子方式相关联。只要克隆在使用中，就不能销毁快照。

快照为什么有用？他们可以防止错误软件或意外删除造成的文件损坏。它们还可以提供在编辑前查看文件的方法。它们可以用作准备备份的文件系统的快照。

克隆为什么有用？克隆的一个有趣用途是在操作系统的重要更新之前创建一个克隆。illumos 和 FreeBSD 早就知道，引导环境是可以引导到的根文件系统克隆。这允许在升级中断后快速重启到已知的正常工作的操作系统。它们也被用作克隆容器和虚拟机的手段。用途受到想象力的限制。

现在，在这个介绍之后，回到用法本身。

## zfs acls

Linux 是 Unix 传统的操作系统。Unix 操作系统是多用户系统，允许许多用户操作同一台计算机。这带来了文件和目录权限控制的标准模型。在这个模型中，有三种类型的动作和三种类型的参与者。动作是读、写和执行，参与者是所有者、组和所有其他人。这两者可以结合起来，提供一种简单而有效的方法来限制和授权对 Linux 中某些目录和文件的访问。这种模式被称为自主访问控制(DAC)。

DAC 允许灵活控制谁可以使用某些系统区域以及如何使用。但是，用户越多，组织结构越复杂，用 DAC 模型表达就越困难。到了某个时候，就变得不可能了。因此，一种新的表示访问控制方法的方式被发明出来。

Linux 采用了 POSIX ACLs。ACL 的意思是访问控制列表，确切地说是:一个访问控制条目的列表，它可以创建更细粒度的策略，关于谁可以读、写或执行给定的文件以及他们如何做。

默认操作系统 illumos 上的 ZFS 支持独立的 ACL 集合，符合 NTFS ACLs。它们由扩展的`ls`和`chmod`命令设置和列出。不幸的是，这些命令与它们在 Linux 上的对应命令不同，因此在 Linux 上，不支持标准的 ZFS ACL。这意味着，如果系统管理员想要超越 DAC 模型，他们必须利用 POSIX ACLs 和标准命令:`setfacl`用于指定列表，而`getfacl`用于列出列表。好处是其他主流 Linux 文件系统都使用这些命令，因此您只需要学习一次。缺点是，如果您从 illumos 或 FreeBSD 导入了一个池，ACL 可能会丢失。

## DAC 模型

在解释 POSIX ACLs 之前，我首先需要使用一个简单的场景来解释 DAC 模型。

假设有一台服务器有三个用户:Alice、John 和 Mikey。爱丽丝是项目经理，约翰是程序员，米奇在会计部门工作。用户可以访问服务器上的三个目录:

*   上面写着:爱丽丝管理的项目的源代码和约翰的代码。公司政策规定，爱丽丝和约翰都应该能够访问该目录的内容，但只有约翰可以添加新文件或编辑现有文件。Mikey 不应该看到这个目录的内容。
*   这个目录包含典型的项目文档。架构分析、项目概述、里程碑、客户签署等。公司政策规定，Mikey 和 John 应该能够读取这些文件，但不能编辑它们，而 Alice 应该能够读取和编辑这些文件。
*   该目录包含财务数据:John 和 Alice 的时间会计、客户发票以及与项目相关的承包商发票、预算等。米奇对这些文件有完全的控制权。Alice 应该可以阅读所有的内容，但只能编辑其中的一部分，John 应该也不能。

显然，这并没有反映真实的编程项目，但是对于我们的目的来说已经足够了。我们拥有的传统 DAC 模型工具有:

*   系统用户和组
*   目录和文件访问控制
*   每个目录和文件都有一个所有者(系统用户)和一个组(也拥有目录或文件的系统组)

有了这三点，我们就可以在这个小场景中做很多关于管理的事情。

让我们从为每个目录创建 ZFS 文件系统开始。假设目录名为`data`。为了获得更好的数据可访问性，池被镜像。

```sh
$ sudo zpool create data mirror /dev/sdb1 /dev/sdc1

```

现在我们有了一个池，我们为三个目录创建文件系统:

```sh
$ sudo zfs create data/Code
$ sudo zfs create data/Documents
$ sudo zfs create data/Accounts

```

假设 alice、john 和 mickey 的系统用户已经存在，并且他们的登录名分别是 Alice、John 和 Mickey。此外，还定义了三个组:`projmgmt`代表项目经理，`devel`代表开发人员，`accnt`代表会计。在我们设置权限之前，让我们创建一个表来准确描述谁应该能够做什么。在建立文件服务器结构以准备这样一个矩阵时，这是一个很好的实践。它有助于整理和想象事物。

访问控制使用三个字母来表示分配给用户或组的权限:

*   `r`–阅读
*   `w`–写
*   `x`–执行。在目录上设置该位意味着用户或组可以看到其内容。你实际上不能执行一个目录。为了区分执行和访问，`x`用于前者，`X`用于后者。

表 [1-1](#Tab1) 很快让人们明白，组与属于它们的用户拥有相同的权利。这样一来，为两者复制访问权限似乎有些矫枉过正。在这个时间点上，它当然是，但是我们应该总是为未来做计划。管理组和用户权限并不需要做很多工作，而且每个目录都需要指定其所属的组。而且，如果将来这些组中的任何一个获得了另一个用户，那么授予他们权限就像将他们添加到他们应该属于的组中一样简单。

表 1-1

Project Directories, Users, Groups, and Access Rights

<colgroup><col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"></colgroup> 
| 用户/组/目录 | 爱丽丝 | 约翰 | 精神 | 项目管理 | 重击 | 重点是 |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| 密码 | 处方药 | 读写执行 | - | 处方药 | 读写执行 | - |
| 文档 | 读写执行 | 处方药 | 处方药 | 读写执行 | 处方药 | 处方药 |
| 帐目 | 处方药 | - | 读写执行 | 处方药 | - | 读写执行 |

这还不包括将运行备份守护程序的单独用户，他们至少应该能够读取所有目录以备份其内容，如果需要，还可以写入并重新创建它们。在本例中，可以通过对目录拍摄快照并使用`zfs send|zfs recv`将它们存储在单独的池中来完成备份，特殊的守护程序可以将它们放在磁带上。

现在，如果我们只想应用用户和所有者的组权限，以下命令就足够了。

```sh
$ sudo chown -R alice:projmgmt data/Documents
$ sudo chown -R john:devel data/Code
$ sudo chown -R mickey:accnt data/Accounts
$ sudo chmod -R =0770 data/Documents
$ sudo chmod -R =0770 data/Code
$ sudo chmod -R =0770 data/Accounts

```

`=0770`是设置权限的八进制模式。等号表示我们希望设置与字符串中完全一样的权限，前导零在这一点上没有意义，第二、第三和第四个数字是所有者、所属组和所有其他人相应的权限。权限集由数字及其总和表示:4-读，2-写，1-执行。任何这些的总和将创建一个唯一的数字:5 表示读取和执行，6 表示读取和写入，7 表示以上所有。八进制模式是一种非常方便的一次性设置所有位的方式。如果我们想要使用命名模式、用户、组或其他模式，我们必须对每个模式运行一次该命令:

```sh
$ sudo chmod -R ug=rwX data/Documents
$ sudo chmod -R o-rwX data/Documents

```

该命令创建表 [1-2](#Tab2) 中反映的权限集。

表 1-2

Project Directories, Users, Groups, and Access Rights After First Commands

<colgroup><col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"></colgroup> 
| 用户/组/目录 | 爱丽丝 | 约翰 | 精神 | 项目管理 | 重击 | 重点是 |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| 密码 | - | 读写执行 | - | - | 读写执行 | - |
| 文档 | 读写执行 | - | - | 读写执行 | - | - |
| 帐目 | - | - | 读写执行 | - | - | 读写执行 |

显然，这不是我们想要达到的目标。解决这个问题的一种方法是将所属组更改为需要读取权限的组:

```sh
$ sudo chown -R john:projmgmt data/Code
$ sudo chmod -R =0750 data/Code

```

这给了 Alice 读取`Code`目录的权限；但是，它没有解决另一个人加入项目管理或会计组的问题。让我们假设 Susan 加入了 PM 团队，并且需要拥有与 Alice 相同的权限集。以目前的模式，这是不可能实现的。这就是 ACL 发挥作用的地方。

## ACL 解释

ZFS 不允许开箱即用地使用 Linux ACLs(或者更确切地说是 POSIX ACLs)。它需要被告知这样做。要运行的命令是:

```sh
$ sudo zfs set acltype=posixacl data

```

这个命令打开给定文件系统的 POSIX ACLs。默认情况下，该属性由所有子文件系统继承，因此如果它设置在 ZFS 的根目录下，它将一直向下传播。您可以通过运行`zfs get`命令来验证它:

```sh
$ sudo zfs get acltype data
NAME  PROPERTY  VALUE     SOURCE
lxd   acltype   posixacl  local

```

ACL 如何帮助解决上述问题？很简单。它们允许开发人员存储比之前使用的三个 DAC 条目更多的内容。可以为每个额外的用户或组设置单独的权限。有两个工具用于管理 ACL:`setfactl`和`getfacl`。

```sh
$ setfacl -m g:projmgmt:r /data/Code
$ setfacl -m g:devel:r /data/Documents
$ setfacl -m g:accnt:r /data/Documents
$ setfacl -m g:projmgmt:r /data/Accounts

```

请记住，ACL 命令是在目录上操作的，而不是在 ZFS 文件系统上操作的！

正如所料，这些命令将给予其他组确切的权限，如表 [1-1](#Tab1) 所示。我们可以通过对每个目录运行`getfacl`来确认，如下所示:

```sh
$ getfacl /data/Documents
getfacl: Removing leading '/' from absolute path names
# file: data/Documents
# owner: alice
# group: projmgmt
user::rwx
group::rwx
group:devel:r--
group:accnt:r--
mask::r-x
other::r-x

```

`setfacl`模式的语法如下:

```sh
setfacl -mode:user|group:permissions directory[/file]

```

`setfacl`命令有两种工作模式:添加 ACL 条目或删除 ACL 条目。相应地使用`-m`和`-x`开关。在前面的示例中，`-m`开关用于向特定组添加 ACL 列表条目。要删除一个条目，您需要使用`-x`开关运行命令:

```sh
$ setfacl -x g:devel /data/Documents

```

这将删除添加到`/data/Documents`目录中的`devel`组的所有 ACL 条目。

## 更换驱动器

在许多情况下，您可能需要更换驱动器。最常见的是驱动器故障。要么您的监控系统警告您驱动器即将出现故障，要么驱动器已经出现故障。无论哪种方式，您都需要向池中添加新的驱动器并删除旧的驱动器。

更换驱动器还有另一个原因。这是一种在不增加驱动器的情况下增加 ZFS 池的方法，缓慢而繁琐——用更大的磁盘一个接一个地替换旧的。

考虑第一种情况。在`zpool`状态输出中，您的池被报告为运行良好，但您知道其中一个驱动器很快会出现故障。假设在下面打印的池中，驱动失败的是`sdb`。

```sh
NAME         STATE    READ   WRITE   CKSUM
tank         ONLINE      0       0       0
  mirror-0   ONLINE      0       0       0
    sdb      ONLINE      0       0       0
    sdc      ONLINE      0       0       0

```

假设您已经将驱动器`sdd`添加到系统中。你可以运行`zpool`替换命令:

```sh
sudo zpool replace tank sdb sdd

```

这将在短时间内将`sdd`附着到`sdb`上，形成一面镜子，然后将`sdb`从水池中移除。或者你可以分两步做，首先手动将`sdd`连接到`sdb`，等到重新安装完成，然后自己拆下`sdb`:

```sh
sudo zpool attach tank sdb sdd

sudo zpool status
  pool: tank
state: ONLINE
  scan: resilvered 114K in 0h0m with 0 errors on Tue Nov 7 21:35:58 2017
config:

NAME         STATE    READ   WRITE   CKSUM
tank         ONLINE      0       0       0
  mirror-0   ONLINE      0       0       0
    sdb      ONLINE      0       0       0
    sdc      ONLINE      0       0       0
    sdd      ONLINE      0       0       0

```

你可以看到，这有效地将镜像 0 变成了三向镜像。监控 resilver 流程，并在完成后发布:

```sh
sudo zpool detach tank sdb

```

这将从您的池中删除`sdb`设备。

如果驱动器已经出现故障，步骤与上述类似，只是您将看到

```sh
NAME         STATE      READ   WRITE   CKSUM
tank         DEGRADED      0       0       0
  mirror-0   DEGRADED      0       0       0
    sdb      UNAVAIL       0       0       0
    sdc      ONLINE        0       0       0

```

遵循先前的步骤:

```sh
sudo zpool replace tank sdb sdd

```

这将用新驱动器替换故障驱动器。

在不添加新驱动器的情况下增加池意味着用新的更大的磁盘替换池中的每个磁盘。假设您想让池槽大于当前的 2 GB:

```sh
sudo zpool list
NAME SIZE  ALLOC FREE  EXPANDSZ FRAG CAP DEDUP HEALTH ALTROOT
tank 1.98G 152K  1.98G -        0%   0%  1.00x ONLINE -

```

这些步骤意味着:

1.  将新驱动器添加到机箱中。它们必须有相同的几何形状和大小。
2.  将更大的新驱动器连接到镜像，并等待它完成重新同步过程。
3.  移除旧驱动器。
4.  连接下一个更大的驱动器。等待 resilver，移除。

您可以运行 replace 命令，而不是附加和移除。它将为您完成上述所有步骤:

```sh
sudo zpool replace tank sdb sdd

```

如果您有由多个 vdev 构建的池，您可以为每个 vdev 运行 replace 命令。这将加快事情的进展。